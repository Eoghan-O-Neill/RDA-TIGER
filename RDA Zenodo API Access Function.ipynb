{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58f8e84",
   "metadata": {},
   "source": [
    "Credentials for the zenodo API take the form of a personal access token which can be created when you create a zenodo account. For full details on how to do this consult the authorization section of the API documentation [here](https://developers.zenodo.org/#authentication). This token is then given as a parameter argument when making a HTTP request. I store my token as a variable called 'token' in a separate .py file and then load that file into python using the below import statement. You can also just create a variable directly in your code but make sure to remove the actual token itself when sharing your code online otherwise others will then be able to access the API using your credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcbc601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zenodo_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f8e6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adc7f5",
   "metadata": {},
   "source": [
    "This function takes 2 arguments the credentials for the API and the date from which you want the resources. It looks a little scary but the majority of it is merely parsing out the data into a tabular format that fits with the current format that we are using in the WP dataset. The function queries the zenodo API for resources from both the ``rda`` and ``rda-related`` communities. A helper function is immediately defined within it that queries the API and then returns sheets of formatted tabular data. These sheets are named in line with their corresponding sheets in the WP dataset. The output of the function therefore will be 4 csv files ``group_resource.csv``, ``resource.csv``, ``individual.csv``, ``individual_resource.csv``. This data can then be manually copied into the google doc or uploaded into an SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3906eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zenodo_scraper(credentials, date):\n",
    "    def helper(community, writetype, header):\n",
    "        r = requests.get(f\"https://zenodo.org/api/records?\", params = {\"access_token\": credentials, \"communities\": community,\n",
    "                                                                      \"sort\": \"mostrecent\"})\n",
    "        \n",
    "        data = r.json()['hits']['hits']\n",
    "        data = [res for res in data if datetime.strptime(res['metadata']['publication_date'], \"%Y-%m-%d\") >= datetime.strptime(date, \"%Y-%m-%d\")]\n",
    "        if len(data) == 0:\n",
    "            return\n",
    "        data = pd.json_normalize(data)\n",
    "\n",
    "        data['Ready'] = np.nan\n",
    "        data['URI_Status'] = data['links.self'].apply(lambda x: requests.head(x).status_code)\n",
    "        data['URI2'] = np.nan\n",
    "        data['URI2_Status'] = np.nan\n",
    "        data['PID_LOD_Type'] = \"DOI\"\n",
    "        data['type'] = data['metadata.resource_type.title'].apply(lambda x: f\"publication-{x}\".lower())\n",
    "        data['dc_type'] = data['metadata.resource_type.title'].apply(lambda x: f\"info:eu-repo/semantics/{x}\".lower())\n",
    "        data['dc_description'] = data['metadata.description'].apply(lambda x: re.sub(r\"<[^>]*>|\\n\", \" \", x).strip())\n",
    "        data['dc_description'] = data['dc_description'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "        data.rename(columns = {'metadata.title': 'Title', 'metadata.publication_date': 'dc_date', \n",
    "                               'metadata.language': 'dc_language', 'links.self_html': 'URI', 'links.doi': 'PID_LOD'}, inplace = True)\n",
    "\n",
    "        data = data[['Title', 'Ready', 'URI', 'URI_Status', 'URI2', 'URI2_Status', 'PID_LOD_Type', 'PID_LOD', 'dc_date', 'dc_description', \n",
    "              'dc_language', 'type', 'dc_type']]\n",
    "\n",
    "        contributors = [x.get('metadata', {}).get('contributors') for x in r.json()['hits']['hits']] \n",
    "        creators = [x.get('metadata', {}).get('creators') for x in r.json()['hits']['hits']] \n",
    "        titles = [x.get('title', {}) for x in r.json()['hits']['hits']]\n",
    "\n",
    "        for x, y in zip(creators, titles):\n",
    "            for z in x:\n",
    "                z['title'] = y\n",
    "\n",
    "        for i, (x, y) in enumerate(zip(contributors, titles)):\n",
    "            if x is None:\n",
    "                continue\n",
    "            else:\n",
    "                for z in x:\n",
    "                    z['title'] = y\n",
    "        contributors = [i for i in contributors if i is not None]\n",
    "\n",
    "        creats = [x for z in creators for x in z]\n",
    "        contribs = [x for z in contributors for x in z]\n",
    "        creats = pd.DataFrame(creats)\n",
    "        contribs = pd.DataFrame(contribs)\n",
    "        contribs.drop(columns = 'type', inplace = True)\n",
    "        individual = pd.concat([creats, contribs], ignore_index=True, sort=False)\n",
    "        individual = individual.loc[~individual['name'].str.contains(r\"WG$|IG$|Working Group|Interest Group|Research Data Alliance\", \n",
    "                                                                na = False)]\n",
    "        individual = individual[['name', 'orcid']]\n",
    "        individual.to_csv('individual.csv', mode = writetype, header = header, index = False)\n",
    "\n",
    "        creat_grps = creats.loc[creats.name.str.contains(r\"WG$|IG$|Working Group|Interest Group\", na = False)]\n",
    "        contribs_grps = contribs.loc[contribs.name.str.contains(r\"WG$|IG$|Working Group|Interest Group\", na = False)]\n",
    "        groups = pd.concat([creat_grps, contribs_grps], ignore_index=True, sort=False)\n",
    "        groups = groups[['name', 'title']]\n",
    "\n",
    "        wgs = groups.loc[groups.name.str.contains(r\"WG$|Working Group\", na = False)]\n",
    "        wgs.rename(columns = {'name': 'WorkingGroupString', 'title': 'Title'}, inplace = True)\n",
    "        igs = groups.loc[groups.name.str.contains(r\"IG$|Interest Group\", na = False)]\n",
    "        igs.rename(columns = {'name': 'InterestGroupString', 'title': 'Title'}, inplace = True)\n",
    "\n",
    "        data = data.merge(wgs, how = 'left', on = 'Title')\n",
    "        data = data.merge(igs, how = 'left', on = 'Title')\n",
    "        data.to_csv('resource.csv', mode = writetype, header = header, index = False)\n",
    "\n",
    "        creats['Relation_UUID'] = \"rda_graph:DB99C7E3\"\n",
    "        creats['relation'] = \"isAuthor\"\n",
    "        creats['UUID_Resource'] = np.nan\n",
    "        contribs['Relation_UUID'] = \"rda_graph:488C40F9\"\n",
    "        contribs['relation'] = \"isContributor\"\n",
    "        contribs['UUID_Resource'] = np.nan\n",
    "\n",
    "        creats.rename(columns = {'name': '(Individual)', 'title': '(Resource)'}, inplace = True)\n",
    "        contribs.rename(columns = {'name': '(Individual)', 'title': '(Resource)'}, inplace = True)\n",
    "        creats = creats[['(Individual)', 'Relation_UUID', 'relation', 'UUID_Resource', '(Resource)']]\n",
    "        contribs = contribs[['(Individual)', 'Relation_UUID', 'relation', 'UUID_Resource', '(Resource)']]\n",
    "\n",
    "        individual_resource = pd.concat([creats, contribs], ignore_index=True, sort=False)\n",
    "        individual_resource.loc[~individual_resource['(Individual)'].str.contains(r\"WG$|IG$|Working Group|Interest Group|Research Data Alliance\", \n",
    "                                                                na = False)].to_csv('individual_resource.csv', mode = writetype, \n",
    "                                                                                    header = header, index = False)\n",
    "\n",
    "        creat_grps['Relation_UUID'] = \"rda_graph:7D9E4FD2\"\n",
    "        creat_grps['relation'] = 'isCreator'\n",
    "        contribs_grps['Relation_UUID'] = \"rda_graph:488C40F9\"\n",
    "        contribs_grps['relation'] = 'isContributor'\n",
    "        creat_grps['UUID_Resource'] = np.nan\n",
    "        contribs_grps['UUID_Resource'] = np.nan\n",
    "        creat_grps.rename(columns = {'name': '(Title_Group)', 'title': '(Title_Resource)'}, inplace = True)\n",
    "        contribs_grps.rename(columns = {'name': '(Title_Group)', 'title': '(Title_Resource)'}, inplace = True)\n",
    "        creat_grps = creat_grps[['(Title_Group)', 'Relation_UUID', 'relation', 'UUID_Resource', '(Title_Resource)']]\n",
    "        contribs_grps = contribs_grps[['(Title_Group)', 'Relation_UUID', 'relation', 'UUID_Resource', '(Title_Resource)']]\n",
    "        group_resource = pd.concat([creat_grps, contribs_grps], ignore_index = True, sort = False)\n",
    "        group_resource.to_csv(\"group_resource.csv\", mode = writetype, header = header, index = False)\n",
    "\n",
    "    communities = [\"rda\", \"rda-related\"]\n",
    "    writetypes = [\"w\", \"a\"]\n",
    "    headers = [True, False]\n",
    "\n",
    "    for x, y, z in zip(communities, writetypes, headers):\n",
    "        helper(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11993e5",
   "metadata": {},
   "source": [
    "The function throws a number of errors. These are related to performing changes to a copy of a data frame and can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a6c7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16776\\184287329.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wgs.rename(columns = {'name': 'WorkingGroupString', 'title': 'Title'}, inplace = True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16776\\184287329.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  igs.rename(columns = {'name': 'InterestGroupString', 'title': 'Title'}, inplace = True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16776\\184287329.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wgs.rename(columns = {'name': 'WorkingGroupString', 'title': 'Title'}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "zenodo_scraper(zenodo_credentials.token, \"2022-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939207cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
