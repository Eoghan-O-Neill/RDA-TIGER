{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2a6090",
   "metadata": {},
   "source": [
    "## Resource Keyword Creation Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e545208",
   "metadata": {},
   "source": [
    "The following code allows us to automatically generate keyword phrases from the text of resources related to the RDA. This process involves the following steps.\n",
    "1. Load in resources as PDFs and convert their the raw text into a list of strings. \n",
    "2. Strip out boilerplate and punctuation from the strings.\n",
    "3. Tokenize or split the strings into candidate phrases.\n",
    "4. Vectorize the candidate phrases removing stopwords, frequent and infrequent phrases, syntactically meaningless phrases.\n",
    "5. Rank the vectorized phrases according to [TF-IDF](https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/) scores and keep a specified number of keyword phrases.\n",
    "6. Manually remove unsuitable phrases.\n",
    "In the broadest sense we create a corpus of documents which then allows us to rank how common phrases are in each document but then weighted for their frequency across the documents. This should (hopefully) return phrases which are maximally representative of each specific document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5618bfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import regex\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978ab610",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_folder = r\"C:\\{Path to folder with resouces}\" # Create a variable with the path to the folder with resources. I would\n",
    "                                                       # recommend naming the files according to their UUIDs as this will make\n",
    "                                                       # it much easier to join the generated keywords to the dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed7c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = [os.path.join(resource_folder, x) for x in os.listdir(resource_folder)] # This creates a list with all the resources\n",
    "                                                                                   # as file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81a8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file: str) -> [str]: # This function takes the text from the PDF and returns it in list format\n",
    "    with open(pdf_file, 'rb') as pdf:\n",
    "        reader = PyPDF2.PdfReader(pdf, strict=False)\n",
    "        pdf_text = []\n",
    "\n",
    "        for page in reader.pages:\n",
    "            content = page.extract_text()\n",
    "            pdf_text.append(content)\n",
    "            \n",
    "        return pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45614e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [] # We create a blank list and then append the text for each resource to it so we then have a list of lists containing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bb9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.00000000000-22737368') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00000000000-45474735') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00000000000-45474735') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00000000000-25579538') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00000000000-25579538') invalid; use 0.0 instead\n",
      "incorrect startxref pointer(1)\n",
      "FloatObject (b'0.00-70') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-70') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-70') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-70') invalid; use 0.0 instead\n"
     ]
    }
   ],
   "source": [
    "for x in resource:\n",
    "    text.append(extract_text_from_pdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = text.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3845c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [str(item) for item in text] # The vectorizer we build later throws an error if its fed a list of lists so we change each\n",
    "                                    # item to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b088eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    re.sub(\"●\", \"\",\n",
    "    re.sub(\"\\s[a-zA-Z]\\.\", \"\",\n",
    "    re.sub(\"\\(\\d+\\)\", \"\",\n",
    "    re.sub(\"10.\\d+/[a-z]+\\d+\", \"\",\n",
    "    re.sub(\"doi:10.\\d+/[a-z]+\\d+\", \"\",\n",
    "    re.sub(\"https?://\\S+\", \"\",\n",
    "    \" \".join(x.replace(\"\\\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\'\", \"\").strip(\"'\").split()).lower()))))))\n",
    "    for x in text\n",
    "] # This list comprehension strips out boilerplate text, URLs and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfcb6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable stores a set of semantic structures which are appropriate keyword formats. \"NN\" for example means Noun Noun. \n",
    "valid_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c1aee",
   "metadata": {},
   "source": [
    "The below function creates a tokenizer class. This tokenizer uses the [Treebank Word Tokenizer](https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html) which deals well with punctuation. As this tokenizer requires sentences rather than raw blocks of text, the function splits the text into sentences first. Each word is also given a part of speech (POS) tag which allows uninformative words and phrase structures to be filtered out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d1c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        result = []\n",
    "        word = r\"\\p{letter}\"\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "            tokens = [t for t in tokens if regex.search(word, t)]\n",
    "            tokens = [t for t in tokens if len(t) > 1]\n",
    "            tokens = [t for t in tokens if \"\\\\\" not in t]\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            tokens = [word for word, tag in pos_tags if tag in valid_pos_tags]\n",
    "            result += tokens\n",
    "        return result\n",
    "    \n",
    "mytokenizer = MyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b20f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This stopword list is custom built. You can add other words or remove items from this list as appropriate. I chose these\n",
    "# specific words simply because inspecting the tokenized text revealed these letters or words to still be present despite them\n",
    "# carrying little meaning.\n",
    "mystopwords = [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w', \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", \n",
    "               'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] + stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa2ab996",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer( \n",
    "    tokenizer=mytokenizer.tokenize, # The Previously defined tokenizer.\n",
    "    stop_words=mystopwords, # The previously defined stopword list.\n",
    "    ngram_range=(2, 3), # This sets the size of the phrases returned. In this case I found phrases a word length of 2 or 3 to be\n",
    "                        # the most informative.\n",
    "    max_df=0.75, # Removes words and phrases that appear in nearly all documents as these are generally uninformative. \n",
    "    min_df=0.005, # Removes very infrequent phrases as these are again usually uninformative.\n",
    "    sublinear_tf=True # This option weights against very frequent terms.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6897e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_w = tfidf_vectorizer.fit_transform(text) \n",
    "d_w # Fitting the vectorizer returns a sparse matrix of terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f61679a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out() # This creates an array of the terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90a8ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the array of features and the sparse matrix this function will return a list of lists with each inner lists containing a\n",
    "# tuple of the top n (specified and changeable by changing the top_n argument) terms and their tf-idf scores.\n",
    "def get_top_keywords(matrix, feature_names, top_n=5):\n",
    "    top_keywords_per_doc = []\n",
    "    for row in matrix:\n",
    "        row_data = row.toarray().flatten()\n",
    "        top_indices = row_data.argsort()[-top_n:][::-1]\n",
    "        top_keywords = [(feature_names[i], row_data[i]) for i in top_indices if row_data[i] > 0]\n",
    "        top_keywords_per_doc.append(top_keywords)\n",
    "    return top_keywords_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "67dbef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = get_top_keywords(d_w, feature_names, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b4d8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[x[0] for x in inner_list] for inner_list in top_keywords] # This removed the tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "53e77315",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"resource\": resource, \"keywords\": keywords}) # This creates a dataframe with resource names and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e7d2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['resource'] = data.resource.str.split(\"\\\\\").str[-1].str.rstrip(\".pdf\") # Clean up the resources and keyword lists.\n",
    "data['keywords'] = data.keywords.astype(str).str.strip(\"[\").str.strip(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out the keyword lists into specific keywords in wide format. \n",
    "data = pd.concat([data['resource'], data['keywords'].str.split(',', expand = True).add_prefix(\"keyword \")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "feca7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars=['resource'], value_vars=['keyword 0', # Pivot the dataset into long format. \n",
    " 'keyword 1',\n",
    " 'keyword 2',\n",
    " 'keyword 3',\n",
    " 'keyword 4',\n",
    " 'keyword 5',\n",
    " 'keyword 6',\n",
    " 'keyword 7',\n",
    " 'keyword 8',\n",
    " 'keyword 9']).sort_values([\"resource\"]).drop(columns = \"variable\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3fb97644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Things for Curating Reproducible and FAIR R...</td>\n",
       "      <td>'research compendium'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Things for Curating Reproducible and FAIR R...</td>\n",
       "      <td>'research artifacts'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 Things for Curating Reproducible and FAIR R...</td>\n",
       "      <td>'air principles'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Things for Curating Reproducible and FAIR R...</td>\n",
       "      <td>'computing environment'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Things for Curating Reproducible and FAIR R...</td>\n",
       "      <td>'things curating'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>presentations from the RDA17 BoF on Addressing...</td>\n",
       "      <td>'data movement'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>presentations from the RDA17 BoF on Addressing...</td>\n",
       "      <td>'early adopters'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>presentations from the RDA17 BoF on Addressing...</td>\n",
       "      <td>'data transfer'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>presentations from the RDA17 BoF on Addressing...</td>\n",
       "      <td>'management systems'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>presentations from the RDA17 BoF on Addressing...</td>\n",
       "      <td>'low level'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               resource  \\\n",
       "0     10 Things for Curating Reproducible and FAIR R...   \n",
       "1     10 Things for Curating Reproducible and FAIR R...   \n",
       "2     10 Things for Curating Reproducible and FAIR R...   \n",
       "3     10 Things for Curating Reproducible and FAIR R...   \n",
       "4     10 Things for Curating Reproducible and FAIR R...   \n",
       "...                                                 ...   \n",
       "2485  presentations from the RDA17 BoF on Addressing...   \n",
       "2486  presentations from the RDA17 BoF on Addressing...   \n",
       "2487  presentations from the RDA17 BoF on Addressing...   \n",
       "2488  presentations from the RDA17 BoF on Addressing...   \n",
       "2489  presentations from the RDA17 BoF on Addressing...   \n",
       "\n",
       "                         value  \n",
       "0        'research compendium'  \n",
       "1         'research artifacts'  \n",
       "2             'air principles'  \n",
       "3      'computing environment'  \n",
       "4            'things curating'  \n",
       "...                        ...  \n",
       "2485           'data movement'  \n",
       "2486          'early adopters'  \n",
       "2487           'data transfer'  \n",
       "2488      'management systems'  \n",
       "2489               'low level'  \n",
       "\n",
       "[2490 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # In the end we have a data frame with resources and keywords. These keywords need to be further manually pruned as not all\n",
    "     # of them will make logical sense. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
